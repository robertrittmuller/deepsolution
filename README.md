# DeepSolution
The ultimate answer to the DeepTraffic MIT reinforcement learning competition!

<img src="images/DeepTraffic-Leaderboard.png">

### Background
MIT <a href="https://github.com/lexfridman/deeptraffic">"Deep Traffic"</a> was a deep reinforcement learning competition that challenges participants to design neural networks capable of navigating a simulated, congested highway. The competition is part of the MIT Deep Learning series—specifically linked with the MIT course “Deep Learning for Self-Driving Cars” (6.S094)—and offers an engaging, hands-on platform where both beginners and experts can experiment with motion planning and hyperparameter tuning. The initiative was run by Lex Fridman, Jack Terwilliger, and Benedikt Jenik, who aimed to inspire innovative approaches to autonomous vehicle navigation by crowdsourcing solutions and showcasing them on a public leaderboard. 

### Introduciton to Q-Learning
Q-learning is a model-free reinforcement learning algorithm that learns an optimal action-value function, Q(s, a), which estimates the expected cumulative reward of taking a given action a in a state s and following the best policy thereafter. The algorithm iteratively updates its Q-values using the temporal difference error defined by the equation Q(s, a) ← Q(s, a) + α [r + γ maxₐ’ Q(s’, a’) − Q(s, a)], where α is the learning rate, γ is the discount factor, r is the immediate reward, and s’ is the subsequent state. In this <a href="https://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html">Stanford ConvNetJS demo</a>, a neural network approximates this Q-function by taking a set of sensory inputs—simulated as “eyes” that measure distances to walls, rewards (apples), and penalties (poison)—and outputs a value for each of the five possible actions the agent can take. The network is trained using backpropagation as the agent interacts with its environment, and it incorporates techniques such as experience replay and an epsilon-greedy policy to balance exploration with exploitation. This setup effectively demonstrates how deep Q-learning can be used to learn optimal behaviors in complex, high-dimensional spaces without requiring an explicit model of the environment.

### How Q-Learning was used in DeepTraffic
In the DeepTraffic competition, deep Q-learning is harnessed to develop neural networks that drive vehicles through congested highways. Participants designed models that ingest a state representation of the highway—such as an occupancy grid capturing the positions and speeds of surrounding vehicles—and output a set of possible driving actions. The neural network approximates the Q-function, predicting the expected cumulative reward for each action based on the current state, and it is trained using temporal difference learning much like in the classic deep Q-learning setup demonstrated in the above Standford ConvNetJS demo. By continuously updating its parameters through interactions with the simulated traffic environment, the algorithm learns to balance aggressive acceleration with safe maneuvering, optimizing the vehicle’s speed while minimizing collision risks. This approach, adapted from early deep reinforcement learning successes in Atari games, underpins the core mechanics of DeepTraffic and encouraged competitors to explore innovative hyperparameter tuning and network architectures to achieve superior driving performance. 
